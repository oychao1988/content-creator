#!/usr/bin/env tsx
/**
 * é€šç”¨ LLM æµ‹è¯•è„šæœ¬
 * æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®
 *
 * ä½¿ç”¨ç¤ºä¾‹:
 *   npx tsx scripts/test-llm.ts "ä½ å¥½"                    # ä½¿ç”¨é»˜è®¤é…ç½®
 *   npx tsx scripts/test-llm.ts "ä½ å¥½" --type api         # ä½¿ç”¨ API æ¨¡å¼
 *   npx tsx scripts/test-llm.ts "ä½ å¥½" --type cli         # ä½¿ç”¨ CLI æ¨¡å¼
 *   npx tsx scripts/test-llm.ts "ä½ å¥½" --no-stream        # ç¦ç”¨æµå¼è¾“å‡º
 *   npx tsx scripts/test-llm.ts "ä½ å¥½" --no-display       # ç¦ç”¨å®æ—¶æ˜¾ç¤º
 */

import { LLMServiceFactory } from '../src/services/llm/LLMServiceFactory.js';
import type { ILLMService } from '../src/services/llm/ILLMService.js';
import { config } from '../src/config/index.js';

// è§£æå‘½ä»¤è¡Œå‚æ•°
const args = process.argv.slice(2);
const prompt = args[0] || 'è¯·ç”¨ä¸€å¥è¯ä»‹ç» TypeScript';

// è§£æé€‰é¡¹
const options = {
  type: 'api' as 'api' | 'cli',
  stream: true,
  display: true,
};

for (let i = 1; i < args.length; i++) {
  const arg = args[i];
  if (arg === '--type' && args[i + 1]) {
    const type = args[++i];
    if (type === 'api' || type === 'cli') {
      options.type = type;
    }
  } else if (arg === '--no-stream') {
    options.stream = false;
  } else if (arg === '--no-display') {
    options.display = false;
  } else if (arg === '--help' || arg === '-h') {
    console.log(`
ä½¿ç”¨æ–¹æ³•:
  npx tsx scripts/test-llm.ts [æç¤ºè¯] [é€‰é¡¹]

å‚æ•°:
  æç¤ºè¯                    è¦å‘é€ç»™ LLM çš„æç¤ºå†…å®¹ï¼ˆé»˜è®¤: "è¯·ç”¨ä¸€å¥è¯ä»‹ç» TypeScript"ï¼‰

é€‰é¡¹:
  --type <api|cli>         LLM æœåŠ¡ç±»å‹ï¼ˆé»˜è®¤: apiï¼‰
  --no-stream              ç¦ç”¨æµå¼è¾“å‡º
  --no-display             ç¦ç”¨å®æ—¶æ˜¾ç¤º
  --help, -h               æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  npx tsx scripts/test-llm.ts "ä½ å¥½"
  npx tsx scripts/test-llm.ts "å†™ä¸€é¦–è¯—" --type api
  npx tsx scripts/test-llm.ts "ä»‹ç» Go" --type cli
  npx tsx scripts/test-llm.ts "test" --no-stream
    `);
    process.exit(0);
  }
}

async function testLLM() {
  console.log('\nğŸ§ª LLM æœåŠ¡æµ‹è¯•');
  console.log('=' .repeat(50));
  console.log(`ğŸ“ æç¤ºè¯: ${prompt}`);
  console.log(`ğŸ”§ é…ç½®: type=${options.type}, stream=${options.stream}, display=${options.display}\n`);

  if (options.display) {
    console.log('â³ æ­£åœ¨ç”Ÿæˆ...\n');
    console.log('ğŸ’¬ å›å¤: ');
  }

  try {
    const startTime = Date.now();

    // æ ¹æ®ç±»å‹åˆ›å»ºæœåŠ¡
    const llmService: ILLMService =
      options.type === 'cli' ? LLMServiceFactory.createCLI() : LLMServiceFactory.createAPI();

    const result = await llmService.chat({
      messages: [{ role: 'user', content: prompt }],
      stream: options.stream,
      enableStreamDisplay: options.display && options.stream,
    });

    const duration = Date.now() - startTime;

    if (!options.display) {
      console.log('\nâœ… ç”ŸæˆæˆåŠŸ!\n');
      console.log(`ğŸ“„ å›å¤å†…å®¹: ${result.content}\n`);
    } else {
      console.log('\nâœ… ç”ŸæˆæˆåŠŸ!\n');
    }

    console.log('ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:');
    console.log(`   - Token ä½¿ç”¨: ${result.usage.totalTokens} (è¾“å…¥: ${result.usage.promptTokens}, è¾“å‡º: ${result.usage.completionTokens})`);
    console.log(`   - è€—æ—¶: ${(duration / 1000).toFixed(2)}s`);
    console.log(`   - æˆæœ¬: $${result.cost.toFixed(6)}\n`);
  } catch (error) {
    console.error('\nâŒ ç”Ÿæˆå¤±è´¥:', error);
    process.exit(1);
  }
}

testLLM();
